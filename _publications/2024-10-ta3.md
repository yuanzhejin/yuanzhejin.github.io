---
title: "TA3: Testing Against Adversarial Attacks on Machine Learning Models"
permalink: /publication/2024-10-ta3
collection: publications
date: 2024-10-06
excerpt: ''
venue: 'arXiv'
authors: 'Yuanzhe Jin, Min Chen'
show_year: false
---
Authors
===
Yuanzhe Jin, Min Chen

Abstract
===
Adversarial attacks are major threats to the deployment of machine learning (ML) models in many applications. Testing ML models against such attacks is becoming an essential step for evaluating and improving ML models. In this paper, we report the design and development of an interactive system for aiding the workflow of Testing Against Adversarial Attacks (TA3). In particular, with TA3, human-in-the-loop (HITL) enables human-steered attack simulation and visualization-assisted attack impact evaluation. While the current version of TA3 focuses on testing decision tree models against adversarial attacks based on the One Pixel Attack Method, it demonstrates the importance of HITL in ML testing and the potential application of HITL to the ML testing workflows for other types of ML models and other types of adversarial attacks.

[Download paper here](https://arxiv.org/abs/2410.05334)
